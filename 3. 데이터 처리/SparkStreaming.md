# Spark Streaming
- SQL 위에서 만들어진 분산 스트림 처리 프로세싱이기 때문에 DataFrame이 거의 강제된다.
  - 즉 DF 처리할 때 사용 가능하다.
  - RDD도 가능하다.
- 데이터 스트림을 처리할 때 사용 가능하다.
- Kafka, HDFS 등 이종 간 연결이 가능하다.
  - 이 중 카프카로 보낸 로그 정보를 스파크로 받아 집계하는 실습 진행 예정
  - 카프카로 데이터를 받아냈으면, 스파크 스트리밍으로 어떻게 할 수 있을지?
- 부분적인 결함이 발생해도 미리 만들어둔 체크포인트로 돌아갈 수 있기에 대응이 수월하다.
- **(데이터가 들어오는) 시간대별 집계가 가능하다.**
## Stream Data
- 데이터 스트림은 무한한 데이터 테이블로, (미래에) 무한한 데이터가 들어올 것을 가정한다.
  - 이미 쌓인 데이터가 batch data
  - 실시간 데이터가 stream data
- 현시점에 들어오는 데이터가 스파크 스트리밍을 통해 배치 데이터가 된다는 개념
- 이 때 데이터를 받아내는 방식이 Micro Batching
- Micro Batching으로 쪼개진 배치는 데이터 프레임, 배치 데이터와 비슷하게 보이는 개념이기 때문에 추후 데이터 프레임처럼 다룰 수 있게 된다.
- 즉 **Micro Batching을 통해 데이터를 받아서 데이터 프레임처럼 다룰 수 있다**는게 스파크 스트리밍의 기조
- 진행 단계
  - 무한한 데이터가 흘러 들어온다. (데이터 스트림)
  - spark streaming이 데이터를 받아 잘개 쪼개서 (마이크로 배칭) spark engine으로 보낸다.
- 쪼개진 배치는 데이터 프레임과 비슷하게 처리 가능하다.
- spark engine을 거쳐서 나온 데이터는 일반적인 데이터 프레임과 같이 저장된다.
## DStream (Discretized Stream)
- RDD와 아주 유사하며 사용방법이 거의 비슷
  - RDD 기반으로 만들어진게 스파크 데이터 프레임
- spark streaming에서 사용하는 데이터 프레임은 DStream을 기반으로 만들어진다.
  - spark stream의 기본적인 추상화
  - 내부적으론 RDD의 연속이고, RDD의 속성을 이어받는다.
  - DStream이 spark streaming의 결과물, 즉 spark streaming에서 나와 spark engine으로 가기 전 단계의 데이터 모습이다. (마이크로 배칭으로 잘게 쪼개진 데이터)
- 불변성과 분산 저장 속성을 이어받아, DStream도 transformations를 거쳐 새로운 DStream을 만들 수 있게 된다.
- 각 batch 별 쪼개진 상태에서 그대로 transformations가 일어나, 시간순으로 정리된 형식의 새로운 DStream이 만들어진다.
데이터 변환을 다 하고 마지막에 액션을 때린다
## Streaming Query: Source
- 데이터를 어디에서 읽어올지 명시한다.
  - 이것만 정하면 끝난다.
- 처리한 데이터 저장 및 기타 스트림에 대한 여러 처리를 수행한다.
  - 여러 데이터 소스를 사용해 join()이나 union()으로 합쳐서 쓸 수 있다.


## 스파크 프로세싱
- 스파크는 기본적으로 **배치 프로세싱**을 지원하는 도구
  - 큰 데이터셋에 대해 한 번 연산을 처리
- spark streaming을 통한 **스트림 프로세싱**도 함께 지원
  - 끝없이 들어오는 데이터의 흐름을 연속적, 준 실시간으로 처리


## 스트림 처리 기법들
1. 레코드 단위 처리 모델
   - 플링크가 대표주자 (다른 툴보단 어렵)
   - 각 노드에서 지속적으로 한 개의 레코드를 받으며, 해당 레코드를 처리하여 생성된 다른 레코드는 또 다음 노드로 보낸다. (실시간)
   - 응답 시간이 아주 짧지만, 높은 처리량을 달성하기 어렵고 특정 노드에 장애가 발생하면 복구하기 어렵다.
   - 실시간 채팅이나 문자중계가 아니면 굳이 레코드 단위 모델 사용하지 않는다.
2. 마이크로 배치 스트림 처리 모델
   - spark streaming에서 기본적으로 사용하는 방식 (대표 주자)
   - 마이크로 배치 스트림 처리 모델만 써도 훌륭하게 처리 가능
   - 스트림 처리를 아주 작은 배치 처리 방식으로 수행한다. 
     - 예를 들어, 유튜브 좋아요 수 집계는, 실시간 처리되어야 하긴 하지만 한사람 한사람이 누를때마다 바로 반응할 필요는 없다. 때문에 일정량 데이터를 조금씩 모아서 실시간으로 반영하는 마이크로 배치 스트림 모델을 일반적으로 많이 사용한다.
   - 높은 처리량이 장점이다.
   - 단점은, 대부분의 데이터 파이프라인에서 시간의 오차가 생길 수 있다는 것 (데이터가 부정확할 수 있다)
     - 예를 들어, 유튜브 댓글에서 좋아요가 적지만 상위군에 올라와 있는 경우가 있다.

## 스파크 스트리밍 종류
- 스파크 프레임워크에서는 배치와 스트리밍을 다루는 코드가 매우 비슷하디.
1. DStream
   - RDD처럼 사용 가능 (RDD API 기반)
   - (RDD와 같이) 개발자들이 작성한 코드와 동일한 순서로 연산 수행 (자동 최적화X)
2. Spark Structured Streaming
   - DataFrame처럼 사용 가능 (가장 베스트 방법)
   - DStream의 단점을 극복하고, streaming processing 코드 작성이 batch processing 코드 작성만큼 쉬워야 한다는 게 기본 원칙
   - 데이터의 스트림을 무한하게 연속적으로 추가하는 데이터의 테이블 개념으로 간주하는 기법이다. 스트리밍 데이터를 테이블이라는 틀로 흘러들어오는 정형화된 데이터로 간주하기 때문에 배치 기본 문법을 모두 사용 가능하다.
   - 결과 테이블이 갱신될 때마다 **세 가지** 기능 제공
     1. append 모드
        - 결과 테이블에 새롭게 추가된 행만 기록
     2. update 모드
        - 결과 테이블에 갱신된 행만 기록
     3. complete 모드
        - 갱신된 전체 테이블을 기록 (모든 데이터)


# 실습